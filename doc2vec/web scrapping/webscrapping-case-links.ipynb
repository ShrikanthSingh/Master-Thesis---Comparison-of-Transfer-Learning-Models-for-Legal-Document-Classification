{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lxml\n",
    "#pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from csv import writer\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_function(url):\n",
    "    website_url = requests.get(url).text\n",
    "    soup = BeautifulSoup(website_url, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def url_construction(pg_start, pg_end, site_name, query_name):\n",
    "    const_url=[]\n",
    "    lead_portion  = 'https://www.law.cornell.edu/search/site/'\n",
    "    trail_portion = '&f%5B0%5D=bundle%3Asupct_node&retain-filters=1&query='\n",
    "    end_portion   = '=onlysyllabi'\n",
    "    for i in range(pg_end):\n",
    "        pg_start_str = str(pg_start)\n",
    "        url = lead_portion + site_name + '?page=' + pg_start_str + trail_portion + query_name + end_portion\n",
    "        const_url.append(url)\n",
    "        pg_start = pg_start+1\n",
    "        \n",
    "    return const_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = requests.get('https://www.law.cornell.edu/supct/topiclist.html').text\n",
    "soup = BeautifulSoup(website_url, 'html.parser')\n",
    "\n",
    "links = []\n",
    "for data in soup.find_all('div', class_='oneOfThree'):\n",
    "    for a in data.find_all('a'):\n",
    "        if a.get('href') != None:\n",
    "            links.append('https://www.law.cornell.edu'+ a.get('href')) #for getting link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_urls =[]\n",
    "category = []\n",
    "i=1\n",
    "for link in links:\n",
    "    print(i)\n",
    "    try:\n",
    "        html_content = soup_function(link)\n",
    "\n",
    "        next_page_url = html_content.find('li', class_='pager-next')\n",
    "        next_page = next_page_url.a['href']\n",
    "\n",
    "        match_next_page = re.match(r\".*?\\?(.*)&f\", next_page)\n",
    "        next_pg_num = re.match(r\"page=(.*)$\", match_next_page.group(1))\n",
    "        pg_start = int(next_pg_num.group(1))\n",
    "        #print(pg_start, type(pg_start))\n",
    "\n",
    "        last_page_url = html_content.find('li', class_='pager-last last')\n",
    "        last_page = last_page_url.a['href']\n",
    "\n",
    "        match_last_page = re.match(r\".*?\\?(.*)&f\", last_page)\n",
    "        last_pg_num = re.match(r\"page=(.*)$\", match_last_page.group(1))\n",
    "        pg_end = int(last_pg_num.group(1))\n",
    "\n",
    "        match_query = re.match(r\".*\\/(.*)\\?p\", next_page)\n",
    "        site_name = match_query.group(1)\n",
    "        query_name = site_name + '&scope'\n",
    "\n",
    "        category.append(site_name)\n",
    "        list_of_urls.append(url_construction(pg_start, pg_end, site_name, query_name))\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "    except AttributeError:\n",
    "        print('Error occusred at: ', link)\n",
    "        pass\n",
    "\n",
    "print(len(list_of_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory - https://www.law.cornell.edu/supct/topiclist.html is arranged in the form of 3 stage structure. Therefore for we breakdown the scrapping of documents in three stages:\n",
    "* Extract different topics on the home page. \n",
    "* Navigate through each topic and store the document links.\n",
    "* Finally select only document links with respect to the labels present in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfUrls = {'URL' : list_of_urls, 'Category' : category}\n",
    "my_df_URL = pd.DataFrame(ListOfUrls)\n",
    "my_df_URL.to_csv('Thesis - Dataset and Transformations/doc2vec/web scrapping files/home_page_topics.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_urls = pd.read_csv('Thesis - Dataset and Transformations/doc2vec/web scrapping files/home_page_topics.csv')\n",
    "topic_urls.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LabelCleaner(text):\n",
    "    Clean_Label = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    return Clean_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name=[]\n",
    "topic_name=[]\n",
    "for idx in range(len(topic_urls)):\n",
    "    print('Running:', idx)\n",
    "    x = ast.literal_eval(topic_urls['URL'][idx])\n",
    "    y = LabelCleaner(topic_urls['Category'][idx])\n",
    "    for url in x:\n",
    "        soup_sub_url = soup_function(url)\n",
    "        h3_list = soup_sub_url.find_all('h3',class_='title')\n",
    "        for h3 in h3_list:\n",
    "            if h3.find('a'):\n",
    "                case_name.append(h3.find('a')['href'])\n",
    "                topic_name.append(y)\n",
    "    print('completed:', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_url_extracted = {'Case URL' : case_name, 'Topic Name' : topic_name}\n",
    "my_df = pd.DataFrame(case_url_extracted)\n",
    "my_df.to_csv('Thesis - Dataset and Transformations/doc2vec/web scrapping files/case_urls_all.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_url_topics = pd.read_csv('Thesis - Dataset and Transformations/doc2vec/web scrapping files/case_urls_all.csv')\n",
    "case_url_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListofLabels = ['immigration', 'habeas', 'insurance', 'health',  'environment or environmental or EPA', \n",
    "                'contracts or assumpsit', 'injury', 'evidence', 'administrative', 'apa', 'schools', \n",
    "                'attorney and fees', 'civil and rights', 'sentencing', 'tax', 'bankruptcy', 'commercial', \n",
    "                'erisa', 'corporations', 'securities', 'property', 'criminal', 'procedure', 'international', \n",
    "                'elections', 'communications', 'drug and related', 'drugs', 'indians', 'workers and compensation', \n",
    "                'antitrust', 'trademark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_URL=[]\n",
    "for idx in range(len(case_url_topics)):\n",
    "    if case_url_topics['Topic Name'][idx] in ListofLabels:\n",
    "        filtered_URL.append(case_url_topics['Case URL'][idx])\n",
    "        \n",
    "selected_URL = {'Filtered Case URL' : filtered_URL}\n",
    "df_url = pd.DataFrame(selected_URL) \n",
    "df_url.to_csv('Thesis - Dataset and Transformations/doc2vec/web scrapping files/case_urls_filtered.csv', index=False, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
