{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import math\n",
    "import spacy\n",
    "import string\n",
    "import glob\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import gensim\n",
    "import gensim.models as g\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_tag_docs(path):\n",
    "    '''\n",
    "    SECTION: 4.6.1 - Training of Paragraph Embedding Model\n",
    "    Function: - Modified form of document tagging. Extends the fixed training window of 10000 tokens to adapt to the length of\n",
    "    the document. The method was explained in the discussion: \n",
    "    https://groups.google.com/g/gensim/c/YPT19ow_4Do/m/NWz56vLeBwAJ?pli=1\n",
    "    '''\n",
    "    file_name=[]\n",
    "    tagged_documents=[]\n",
    "    doc_count = 0\n",
    "\n",
    "    for file in glob.glob(path):\n",
    "        with open(file, 'r', encoding='utf8', errors= 'ignore') as infile:\n",
    "            file_name.append(file)\n",
    "            doc_content = infile.read().split()\n",
    "            len_content = len(doc_content)\n",
    "            if len_content <= 10000:\n",
    "                tagged_documents.append(TaggedDocument(words=doc_content,tags=[doc_count]))\n",
    "                doc_count = doc_count + 1\n",
    "                print('Tagging document completed for', doc_count)\n",
    "\n",
    "            else:\n",
    "                limit_per_doc = 10000\n",
    "                factor = math.ceil(len_content/limit_per_doc)\n",
    "                for i in range(factor):\n",
    "                    if i==0:\n",
    "                        tagged_documents.append(TaggedDocument(words=doc_content[:limit_per_doc],tags=[doc_count]))\n",
    "                    else:\n",
    "                        tagged_documents.append(TaggedDocument(words=doc_content[limit_per_doc:(i+1)*10000],tags=[doc_count]))\n",
    "                        if i != factor - 1:\n",
    "                            limit_per_doc = limit_per_doc + 10000\n",
    "                        else:\n",
    "                            doc_count = doc_count + 1\n",
    "                print('Tagging document completed for', doc_count)\n",
    "                \n",
    "    return tagged_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Hyper-parameters\n",
    "**dm ({1,0}, optional)** – Defines the training algorithm. If dm=1, ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
    "* Choice of parameter - (dm = 0)\n",
    "* Reason: https://www.aclweb.org/anthology/W16-1609.pdf\n",
    "* Our findings reveal that dbow, despite being the simpler model, is superior to dmpv. \n",
    "* dbow favours longer windows for context words than dmpv.\n",
    "* dmpv also requires more training epochs than dbow.\n",
    "\n",
    "**vector_size (int, optional)** – Dimensionality of the feature vectors.\n",
    "* Choice of parameter - vector size = 300\n",
    "* Reason - https://www.aclweb.org/anthology/W16-1609.pdf\n",
    "\n",
    "**window (int, optional)** – The maximum distance between the current and predicted word within a sentence.\n",
    "* Choice of parameter - Window = 5\n",
    "* If window = 4, your context is w-4, w-3, w-2, w-1, CENTER_WORD , w+1, w+2, w+3, w+4 (without center word).\n",
    "\n",
    "**min_count (int, optional)** – Ignores all words with total frequency lower than this.\n",
    "* Choice of parameter - min_count = 1\n",
    "* Reason - https://groups.google.com/forum/#!topic/gensim/xKvUv-yZI2U\n",
    "* As your dataset & vocabulary grow, words with just a few instances may be less interesting, and the added model size for a larger vocabulary may become a concern, so increasing min_count can make sense. \n",
    "\n",
    "**sample (float, optional)** – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "* Choice of parameter - 1e-05\n",
    "* Reason - https://groups.google.com/forum/#!topic/gensim/xKvUv-yZI2U\n",
    "* Larger corpuses may benefit from a more-aggressive sample parameter (smaller value, eg 1e-05 or 1e-06), discarding more of the most-frequent words – and thus perhaps improving influence of less-common words, or freeing time for more passes or expansion of other parameters that would otherwise slow training.\n",
    "\n",
    "**epochs (int, optional)** – Number of iterations (epochs) over the corpus.\n",
    "* Choice of parameter - 20\n",
    "* Reason - https://groups.google.com/forum/#!topic/gensim/xKvUv-yZI2U\n",
    "* An epochs value of 10-20 is still likely to be a good starting point, perhaps trying more if your own evaluations can confirm improvement.\n",
    "\n",
    "**hs or negative sampling** \n",
    "* Reason - https://stackoverflow.com/questions/46860197/doc2vec-and-word2vec-with-negative-sampling\n",
    "* Hierarchical-softmax tends to get slower with larger vocabularies (because the average number of nodes involved in each training-example grows); negative-sampling does not (because it's always N+1 nodes). Projects with larger corpuses tend to trend towards preferring negative-sampling.\n",
    "\n",
    "**hs ({1,0}, optional)** – If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
    "* Reason - https://groups.google.com/forum/#!topic/gensim/9EaJAl95cPw\n",
    "* They are two alternative options for how to calculate the predictions of the neural-network, and thus also the errors to be back-propagated. They are relevant options for both Word2Vec (both CBOW or Skip-Gram) and Doc2Vec (both DBOW and DM). Neither have any effect on whether `window` is consulted: `negative`-vs-`hs` are ways to construct/interpret the NN output.\n",
    "\n",
    "**negative (int, optional)** – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "* Choice: negative = 5\n",
    "* Reason - http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "* The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build_train(tagged_documents):\n",
    "    model= Doc2Vec(dm=0, vector_size=300, window_size = 5, sampling_threshold = 1e-5, \n",
    "                   negative=5, hs=0, min_count=1, workers=cores, epochs=20, alpha = 0.025, \n",
    "                   min_alpha = 0.00025, ns_exponent=0.75)\n",
    "\n",
    "    model.build_vocab([x for x in tqdm(tagged_documents)])\n",
    "    model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save(r'C:\\Users\\Shrikanth Singh\\Desktop\\Thesis-Note-to-Py\\doc2vec.model')\n",
    "    print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = r\"C:\\Users\\Shrikanth Singh\\Desktop\\Thesis-Note-to-Py\\sample files for word2vec\\*\"\n",
    "    tagged_documents = frame_tag_docs(path)\n",
    "    model_build_train(tagged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
