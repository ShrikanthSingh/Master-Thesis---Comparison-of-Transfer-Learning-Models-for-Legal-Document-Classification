{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install this library to run the functions offered by LexNLP frame work\n",
    "#pip install --user lexnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from itertools import chain \n",
    "nlp = en_core_web_sm.load()\n",
    "from nltk.corpus import wordnet\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing specific functions from LexNLP \n",
    "import lexnlp.extract.en.acts\n",
    "import lexnlp.extract.en.dates\n",
    "import lexnlp.extract.en.courts\n",
    "import lexnlp.extract.en.trademarks\n",
    "import lexnlp.extract.en.regulations\n",
    "import lexnlp.extract.en.entities.nltk_re\n",
    "from lexnlp.extract.en.addresses import address_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(raw_text):\n",
    "    '''\n",
    "    Function: Basic text cleaning, eliminating any text that contains date and time.\n",
    "    '''\n",
    "    text_string = raw_text.replace('\\n', '.')\n",
    "    text_string = text_string.replace('..', '.')\n",
    "    text_string  = re.sub(r'[^a-zA-Z\\.]', ' ', text_string)\n",
    "    \n",
    "    content_salutations_removed = ' '\n",
    "    salutations = [\"Ms\", \"Mr\", \"mr\", \"ms\"]\n",
    "    temp_salutations_removed = ' '.join([word for word in text_string.split() if word not in salutations])\n",
    "    content_salutations_removed = content_salutations_removed + temp_salutations_removed\n",
    "    def is_valid_date(date_str):\n",
    "        try:\n",
    "            parser.parse(date_str)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    content_dates_removed = ' '.join([w for w in content_salutations_removed.split() if not is_valid_date(w)])\n",
    "    content_lower_cased = content_dates_removed.lower()\n",
    "    return content_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_lemma(raw_text):\n",
    "    '''\n",
    "    Function: Lemmatization and Stop word removal using SpaCy library. \n",
    "    In addition remove initials in front of names eg: J. Carry Kriston --> Carry Kriston.\n",
    "    '''\n",
    "    content_lower_cased =  cleaning_text(raw_text)\n",
    "    content_with_stopwords = nlp(content_lower_cased)\n",
    "    content_without_stopwords = ' '.join([str(token) for token in content_with_stopwords if not token.is_stop])\n",
    "\n",
    "    content_without_lemmatized = nlp(content_without_stopwords)\n",
    "    content_lemmatized = ' '.join([str(token.lemma_) for token in content_without_lemmatized])\n",
    "    \n",
    "    preprocessed_text = re.sub(r'\\b\\w{1}\\b', '', content_lemmatized)\n",
    "    text = re.sub(r'\\.(?! )', '. ', re.sub(r' +', ' ', preprocessed_text))\n",
    "    clean_text = re.sub(r'(\\s+\\.+)+', \".\", text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _createsentences(raw_text):\n",
    "    '''\n",
    "    Function: Remove salutations and align sentence periods for accurate sentence tokenization in the summarization phase. \n",
    "    Remove chosen Named Entities from the document like Name, Organization, Geo-political and Ordinal entities.\n",
    "    '''\n",
    "    text = stop_lemma(raw_text)\n",
    "    punkt_params = PunktParameters()\n",
    "    punkt_params.abbrev_types = set(['dr', 'vs', 'mr', 'mrs', 'ms', 'prof', 'mt', 'inc', 'i.e', 'e.g', \n",
    "                                     'U.S.C.', 'U.S.', 'i.e', 'viz', 'Id.', 'Act'])\n",
    "    sentence_splitter = PunktSentenceTokenizer(punkt_params)\n",
    "    text = re.sub(r'\\.(?! )', '. ', re.sub(r' +', ' ', text))\n",
    "    text_unprocessed = text.replace('?\"', '? \"').replace('!\"', '! \"').replace('.\"', '. \"')\n",
    "    text_unprocessed = text_unprocessed.replace('\\n', ' . ')\n",
    "    unprocessed_sentences = sentence_splitter.tokenize(text_unprocessed)\n",
    "    \n",
    "    for ndx, sentence in enumerate(unprocessed_sentences):\n",
    "        sentence = sentence.replace('? \" ', '?\" ').replace('! \" ', '!\" ').replace('. \" ', '.\" ')\n",
    "        sentence = sentence[:-2] if (sentence.endswith(' .') or sentence.endswith(' . ')) else sentence\n",
    "        unprocessed_sentences[ndx] = sentence\n",
    " \n",
    "    filter_sentences = [i for i in range(len(unprocessed_sentences))\n",
    "                                if len(unprocessed_sentences[i].replace('. ', '').split(' ')) >= 8]\n",
    "    unprocessed_sentences = [unprocessed_sentences[i] for i in filter_sentences]\n",
    "    processed_sentences = ''.join(unprocessed_sentences)\n",
    "\n",
    "    text_no_namedentities = []\n",
    "    text_NER = nlp(processed_sentences)\n",
    "    Entity_Labels = ['PERSON','ORG', 'GPE', 'LOC', 'ORDINAL']\n",
    "    ents = [e.text for e in text_NER.ents if e.label_ in Entity_Labels]\n",
    "    for item in text_NER:\n",
    "        if item.text in ents:\n",
    "            pass\n",
    "        else:\n",
    "            text_no_namedentities.append(item.text)\n",
    "    \n",
    "    text_without_entities = \" \".join(text_no_namedentities)\n",
    "\n",
    "    Ready_text = re.sub(' +', ' ', text_without_entities)\n",
    "    Ready_text = re.sub('\\.','. ',Ready_text)\n",
    "    Ready_text = re.sub(r'\\b\\w{1}\\b', '', Ready_text)\n",
    "    return Ready_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def court_data_removal(Content):\n",
    "    '''\n",
    "    Function: Find out presence of any legal entities (Acts, Regulations, Trademarks and Court Names) \n",
    "    from the document and eliminate it.\n",
    "    '''\n",
    "    clean_content = _createsentences(Content)\n",
    "    TextWithoutComp = clean_content\n",
    "    companies = [str(comp).lower() for comp in list(lexnlp.extract.en.entities.nltk_re.get_companies(TextWithoutComp))]\n",
    "    if companies:\n",
    "        Start_idx, End_idx = [], []\n",
    "        for comp in companies:\n",
    "            Name_Indices =  comp.split(',')\n",
    "            Start_idx.append(int(Name_Indices[1].strip(' (')))\n",
    "            End_idx.append(int(Name_Indices[2].strip(')')))\n",
    "\n",
    "        Strip_Indices = []\n",
    "        for begin, end in zip(Start_idx, End_idx):\n",
    "            Strip_Indices.append([begin, end])\n",
    "\n",
    "        lst = list(TextWithoutComp)\n",
    "        for idx in Strip_Indices[::-1]: \n",
    "            del lst[slice(*idx)]\n",
    "        text = ''.join(lst)\n",
    "        TextWithoutComp = re.sub(' +', ' ', text)\n",
    "            \n",
    "    TextWithoutActs = TextWithoutComp\n",
    "    acts = lexnlp.extract.en.acts.get_act_list(TextWithoutActs)\n",
    "    act_from_data = []\n",
    "    if acts:\n",
    "        for act in acts:\n",
    "            Name_Act = act.get('value').lower()\n",
    "            act_from_data.append(Name_Act)\n",
    "            TextWithoutActs = ''.join(TextWithoutActs.split(Name_Act))\n",
    "\n",
    "    TextWithoutTrademarks = TextWithoutActs\n",
    "    trademarks = list(lexnlp.extract.en.trademarks.get_trademarks(TextWithoutTrademarks))\n",
    "    if trademarks:\n",
    "        for trademark in trademarks:\n",
    "            TextWithoutTrademarks = ''.join(TextWithoutTrademarks.split(trademark.lower()))\n",
    "\n",
    "    TextWithoutRegulations = TextWithoutTrademarks\n",
    "    regulations = [x[1] for x in list(lexnlp.extract.en.regulations.get_regulations(clean_content))]\n",
    "    if regulations:\n",
    "        for reg in regulations:\n",
    "            TextWithoutRegulations = ''.join(TextWithoutRegulations.split(reg.lower()))\n",
    "\n",
    "    TextWithoutCourtName = TextWithoutRegulations\n",
    "    data_of_court = []\n",
    "    for court_data in court_name_alias:\n",
    "        if clean_content.find(court_data) != -1:\n",
    "            data_of_court.append(court_data)\n",
    "            TextWithoutCourtName = ''.join(TextWithoutCourtName.split(court_data))\n",
    "\n",
    "    return TextWithoutCourtName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_court_name():\n",
    "    '''\n",
    "    Function: Fetching court names from the Unites States court database maintained by LexNLP. \n",
    "    '''\n",
    "    court_df = pd.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\n",
    "    court_name, court_alias = [], []\n",
    "    for _, row in court_df.iterrows():\n",
    "        court_name.append(row[\"Court Name\"])\n",
    "        court_alias.extend(row[\"Alias\"].split(\";\"))\n",
    "    court_name_alias = [x.lower() for x in list(chain(court_name, court_alias))]\n",
    "    return court_name_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_pipe(input_data_dir):\n",
    "    '''\n",
    "    Function: Final function that takes the actual dataset and applies all the function to produce the cleanest data possible.\n",
    "    '''\n",
    "    \n",
    "    data = load_files(input_data_dir, encoding=\"utf-8\", decode_error=\"replace\")\n",
    "    labels, counts = np.unique(data.target, return_counts=True)\n",
    "    labels_str = np.array(data.target_names)[labels]\n",
    "    \n",
    "    raw_text, case_labels, file_name = [], [], []\n",
    "    num_of_files = len(data.data)\n",
    "    for i in range(num_of_files):\n",
    "        print('Completed for {} files'.format(i))\n",
    "        raw_text.append(court_data_removal(data.data[i])) \n",
    "        case_labels.append(data.target_names[data.target[i]])\n",
    "        file_name.append(data.filenames[i])\n",
    "        \n",
    "    text_Cleaned = {'Case_document' : raw_text, 'Case_label' : case_labels, 'Case_filename' : file_name}\n",
    "    my_df = pd.DataFrame(text_Cleaned)\n",
    "    my_df.to_csv('Thesis - Dataset and Transformations/transform - post preprocessing/fully_preprocessed_dataset.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    court_name_alias = get_court_name()\n",
    "    input_data_dir = 'Thesis - Dataset and Transformations/actual dataset/semi_preprocessed_cases.zip'\n",
    "    pre_process_pipe(input_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
