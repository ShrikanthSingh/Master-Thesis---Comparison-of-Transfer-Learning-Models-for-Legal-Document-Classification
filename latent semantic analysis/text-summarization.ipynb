{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    \n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from numpy.linalg import svd as svd\n",
    "from scipy.sparse.linalg import svds\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    list_of_sentences = nltk.sent_tokenize(text)\n",
    "    return list_of_sentences\n",
    "    \n",
    "\n",
    "def _compute_matrix(sentences, weighting, norm): \n",
    "    \n",
    "    '''\n",
    "    SECTION - 4.2.3 - Input Matrix Creation\n",
    "    \n",
    "    Compute the matrix of term frequencies or tfidf or binary representation given a list of sentences\n",
    "    \n",
    "    :param ngram_range - The lower and upper boundary of the range of n-values for different word n-grams or char n-grams \n",
    "                         to be extracted. All values of n such such that min_n <= n <= max_n will be used. \n",
    "                         For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "\n",
    "    :param min_df -      When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "\n",
    "    :param binary -      If True, all non zero counts are set to 1. \n",
    "\n",
    "    :param max_df -      To remove intra corpus detected stop words\n",
    "\n",
    "    '''\n",
    "        \n",
    "    if weighting.lower() == 'binary':\n",
    "        vectorizer = CountVectorizer(min_df=1, ngram_range=(1, 2), binary=True, stop_words=None)\n",
    "    elif weighting.lower() == 'frequency':\n",
    "        vectorizer = CountVectorizer(min_df=1, ngram_range=(1, 2), binary=False, decode_error = 'ignore', stop_words='english')\n",
    "    elif weighting.lower() == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2), stop_words='english', norm = 'l2', decode_error = 'ignore')\n",
    "    else:\n",
    "        raise ValueError('Parameter \"method\" must take one of the values \"binary\", \"frequency\" or \"tfidf\".')\n",
    "\n",
    "    frequency_matrix = vectorizer.fit_transform(sentences).astype(float)\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Normalize the term vectors (i.e. each row (axis=1) adds to 1). \n",
    "    # Hence each row is considered as a vector and for normalizing it to unit vector length we use l2 norm.\n",
    "    # l2 norm is simply the distance in euclidean space. Scale input vectors individually to unit norm (vector length).\n",
    "    if weighting.lower() == 'binary' or weighting.lower() == 'frequency':\n",
    "        if norm in ('l1', 'l2'):\n",
    "            frequency_matrix = normalize(frequency_matrix, norm=norm, axis=0)\n",
    "        elif norm is not None:\n",
    "            raise ValueError('Parameter \"norm\" can only take values \"l1\", \"l2\" or None')\n",
    "\n",
    "    return frequency_matrix, terms\n",
    "    \n",
    "    \n",
    "def SVD_cal(raw_text, threshold_value, num_of_sentences,  weighting, norm='l2'):\n",
    "    \n",
    "    '''\n",
    "    SECTION - 4.2.4 - Singular Value Decomposition\n",
    "    \n",
    "    param - threshold_value: Apply a threshold based approach to remove singular values less than a heuristic value \n",
    "    (must be between 0 and 1) of the largest singular values\n",
    "    \n",
    "    param - weighting: mode of sentence matrix creation either tf-idf, tf or binary representation.\n",
    "    \n",
    "    param - num_of_sentence: Selecting number of sentences to be selected for summarization.\n",
    "    '''\n",
    "\n",
    "    token_sentences = tokenize_sentences(raw_text)\n",
    "    sentence_matrix, feature_names = _compute_matrix(token_sentences, weighting, norm=norm)\n",
    "    sentence_matrix = sentence_matrix.transpose()\n",
    "    sentence_matrix = sentence_matrix.multiply(sentence_matrix > 0)\n",
    "    num_of_topics = min(sentence_matrix.shape) - 1\n",
    "\n",
    "    # To make the output non-varying.svds uses random intial vector from dimension N of the sparse matrix. \n",
    "    # So to set the initial vector to a constant choice we must use the v0 parameter and the code is mentioned below.\n",
    "    np.random.seed(0)\n",
    "    v0 = np.random.rand(min(sentence_matrix.shape))\n",
    "    \n",
    "    '''\n",
    "    :param sentence_matrix - Array to compute the SVD on, of shape (M, N)\n",
    "    :param k - Number of largest singular values to be considered for summary generation. Usually k < min(sentence_matrix.shape)\n",
    "    :param v0 - Initialization of vector for iterations. By default is is random but has to be fixed in order to generate the contant results.\n",
    "    :param which - Which k singular values to find: ‘LM’ : largest singular values ‘SM’ : smallest singular values\n",
    "    '''\n",
    "    u, s, v = svds(sentence_matrix, k=num_of_topics, v0=v0, which='LM')\n",
    "    \n",
    "    # A preprocessing step is embedded between the SVD and sentence selection process.\n",
    "    # The first average sentence score is calculated for each concept which is represented by a row of VT matrix.\n",
    "    # If the value of a cell in that row is less than the calculated average score of that row, the score in the cell is set to zero\n",
    "    \n",
    "    topic_averages = v.mean(axis=1)\n",
    "    topic_sigma_threshold = threshold_value\n",
    "    for topic_ndx, topic_avg in enumerate(topic_averages):\n",
    "        v[topic_ndx, v[topic_ndx, :] <= topic_avg] = 0  \n",
    "    \n",
    "    if 1 <= topic_sigma_threshold < 0:\n",
    "        raise ValueError('Parameter topic_sigma_threshold must take a value between 0 and 1')\n",
    "    \n",
    "    # Apply a threshold-based approach to remove singular values that are less than half of the largest singular value if any exist. \n",
    "    # This is a heuristic, and you can play around with this value if you want.\n",
    "    # Mathematically, Si = 0 iff Si < (1/2)max(S). \n",
    "    \n",
    "    sigma_threshold = max(s) * topic_sigma_threshold\n",
    "    s[s < sigma_threshold] = 0  \n",
    "    \n",
    "    # Build a \"length vector\" containing the length (i.e. saliency) of each sentence.\n",
    "    # Multiply each term sentence column from V squared with its corresponding singular value from S also squared, to get sentence weights per topic.\n",
    "    # Compute the sum of the sentence weights across the topics and take the square root of the final score to get \n",
    "    # the salience scores for each sentence in the document.\n",
    "    \n",
    "    saliency_vec = np.sqrt(np.dot(np.square(s), np.square(v)))\n",
    "    top_sentences = saliency_vec.argsort()[-num_of_sentences:][::-1]\n",
    "    \n",
    "    # Once we have these scores, we sort them in descending order, pick the top n sentences corresponding to the highest scores.\n",
    "    top_sentences.sort()\n",
    "    \n",
    "    # Return the sentences in the order in which they appear in the document.\n",
    "    list_summary = [token_sentences[i] for i in top_sentences]\n",
    "    string_summary = \" \".join(str(item) for item in list_summary)\n",
    "\n",
    "    return string_summary\n",
    "    \n",
    "\n",
    "def num_of_sentences(text_input):\n",
    "    '''\n",
    "    SECTION 5.2, EQUATION 5.1\n",
    "    Function - Defines the number of sentences to be chosen for the document summarization based on the number of sentences in\n",
    "    the actual document and the number of tokens in each sentences.\n",
    "    '''\n",
    "    tokenized_sentences = tokenize_sentences(text_input)\n",
    "    avg_words_per_sentence = mean([len(sentence.split()) for sentence in tokenized_sentences])\n",
    "    return math.ceil(500/avg_words_per_sentence)\n",
    "    \n",
    "    \n",
    "def summarization_process(Structured_dataset, weighting):\n",
    "    Threshold_Sigma, Errorneous_files  = 0.8, 0\n",
    "    LSA_Text, Case_Label, Doc_ID, Legal_Details, Errorneous_files_list, filename = [], [], [], [], [], []\n",
    "    for idx, each_doc in Structured_dataset.iterrows():\n",
    "        try:     \n",
    "            Num_of_Sentences = num_of_sentences(each_doc['Preprocessed_Text'])\n",
    "            print('Executing {} file and Number of sentences to select {} out of {}'\n",
    "                  .format(idx, Num_of_Sentences, len(tokenize_sentences(each_doc['Preprocessed_Text']))))\n",
    "\n",
    "            LSA_Text.append(SVD_cal(each_doc['Preprocessed_Text'], Threshold_Sigma, Num_of_Sentences, weighting))\n",
    "            Case_Label.append(each_doc['Case_Label'])\n",
    "            Doc_ID.append(each_doc['DocID'])\n",
    "            Legal_Details.append(each_doc['Legal_Details'])\n",
    "            filename.append(each_doc['File_Name'])\n",
    "\n",
    "        except ValueError:\n",
    "            Errorneous_files = Errorneous_files + 1\n",
    "            Errorneous_files_list.extend(idx, doc_content)\n",
    "            pass\n",
    "    \n",
    "    text_BERT_format = {'Summarized_content' : LSA_Text, 'Labels' : Case_Label, \"DocID\" : Doc_ID, \n",
    "                        'Legal_Details' : Legal_Details, 'Filename':filename}\n",
    "    my_df = pd.DataFrame(text_BERT_format)\n",
    "    \n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    Structured_dataset = pd.read_csv('Thesis - Dataset and Transformations/transform - post legal data extraction/fully_preprocessed_with_legal_entites.csv')\n",
    "    summarized_documents_df = summarization_process(Structured_dataset, weighting='binary')\n",
    "    # Change the saving path:\n",
    "    # For tfidf: Thesis - Dataset and Transformations/transform - post summarization/LSA_tfidf.csv\n",
    "    # For term freq: Thesis - Dataset and Transformations/transform - post summarization/LSA_frequency.csv\n",
    "    summarized_documents_df.to_csv('Thesis - Dataset and Transformations/transform - post summarization/LSA_binary.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
