{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy --user\n",
    "#!pip install torch>=1.2.0 transformers>=2.5.0 --user\n",
    "#!pip install nlpaug numpy matplotlib python-dotenv --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shrikanth\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shrikanth\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation Operations  - SECTION 4.4.3\n",
    "* Random Noise Injection - Random Swap, Random Insertion, Random Deletion.\n",
    "* Synonym Replacement\n",
    "* Similar Word Replacement\n",
    "* Context Word Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "\n",
    "\n",
    "# Augmentation parameter Î± that define how much augmentation should be performed per sentence in the document.\n",
    "\n",
    "alpha_sr=0.2\n",
    "alpha_ri=0.2\n",
    "alpha_rs=0.2\n",
    "alpha_swr=0.2\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "# Finds  the  synonym  of n random  words  and  insert  them  at random position in the sentence\n",
    "# Details about sysnet - https://www.nltk.org/howto/wordnet.html\n",
    "########################################################################\n",
    "def synonym_replacement(words):\n",
    "    num_words = len(words)   \n",
    "    n = max(1, int(alpha_sr*num_words))\n",
    "    new_words = words.copy()\n",
    "    num_replaced = 0\n",
    "    for random_word in new_words:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: \n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    text_augmentation_pipeline.Augmented_sentences.append(sentence)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "# Two words are chosen from a sentence randomly and their positions are swapped\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words):\n",
    "    num_words = len(words)\n",
    "    n = max(1, int(alpha_rs*num_words))\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "  \n",
    "    sentence = ' '.join(new_words)\n",
    "    text_augmentation_pipeline.Augmented_sentences.append(sentence)\n",
    "\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words):\n",
    "    num_words = len(words)\n",
    "    n = max(1, int(alpha_ri*num_words))\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    sentence = ' '.join(new_words)\n",
    "    text_augmentation_pipeline.Augmented_sentences.append(sentence)\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "    \n",
    "#########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "#########################################################################\n",
    "\n",
    "def random_deletion(words, p=0.1):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "    sentence = ' '.join(new_words)\n",
    "    text_augmentation_pipeline.Augmented_sentences.append(sentence)\n",
    "\n",
    "    \n",
    "#################################################################################\n",
    "# Replacement by most similar words\n",
    "# Choose a random word in the sentence and replace it with the most similar word\n",
    "#################################################################################\n",
    "\n",
    "def similar_word_replacement(words):\n",
    "    alpha_swr = 0.2\n",
    "    num_words = len(words)\n",
    "    n = max(1, int(alpha_swr*num_words))\n",
    "    new_words = words.copy()\n",
    "    num_replaced = 0\n",
    "    for random_word in new_words:\n",
    "        try:\n",
    "            most_similar_words = [similar_word[0] for similar_word in Word2Vec_model .wv.most_similar(positive=[random_word])]\n",
    "            filtered_similar_words = list(filter(lambda x: len(x) > 3, most_similar_words))\n",
    "            if len(filtered_similar_words) > 0:\n",
    "                most_similar_word = filtered_similar_words[0]\n",
    "                new_words = [most_similar_word if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "                if num_replaced >= n: #only replace up to n words\n",
    "                    break\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    text_augmentation_pipeline.Augmented_sentences.append(sentence)\n",
    "\n",
    "#####################################################################\n",
    "# Transformer based model BERT trained on large amount of text data \n",
    "# by one of the pretraining strategies called Masked Language using \n",
    "# which certain portions of text can be masked and then predicted.\n",
    "######################################################################\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
    "def context_word_substitute(words):\n",
    "    augmented_text = aug.augment(words)\n",
    "    Augmented_sentences.append(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of equation - 4.8 - Number of Augmentation Set Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_set_calculator(Sumy_csv):\n",
    "    Sumy_Label_df = Sumy_csv['Labels'].to_frame()\n",
    "    Sumy_Label_df = Sumy_Label_df.groupby(Sumy_Label_df.columns.tolist()).size().reset_index().rename(columns={0:'Records'})\n",
    "    Ni_max = Sumy_Label_df.Records.max()\n",
    "    n_aug = []\n",
    "    for idx, row in Sumy_Label_df.iterrows():\n",
    "        n_aug.append(math.floor((Ni_max / row['Records']) - 1))\n",
    "    \n",
    "    Sumy_Label_df['n_aug'] = n_aug\n",
    "    return Sumy_Label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of figure 4.5: Text Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_augmentation_pipeline(df, type_df):\n",
    "    '''\n",
    "    param df: Input dataframe for which augmentation have to be performed.\n",
    "    param type_df: Choice supplied w.r.t value of n_aug. If n_aug = 0 then type_df == 'WAS' else type_df== 'WoAS'.\n",
    "    '''\n",
    "    Df_Augmented_files = pd.DataFrame()\n",
    "    count = 1\n",
    "    for idx,row in df.iterrows():\n",
    "        print('Completed for', count)\n",
    "        count = count + 1\n",
    "        \n",
    "        if type_df == 'WAS': # With Augmented set\n",
    "            num_augmented_set = Sumy_Label_df.loc[Sumy_Label_df['Labels'] == row['Labels'], 'n_aug'].iloc[0]\n",
    "        elif type_df== 'WoAS': #Without Augmented set\n",
    "            num_augmented_set = 1\n",
    "        \n",
    "        tokenized_text = []\n",
    "        list_sentences=[]\n",
    "        sent_text = nltk.sent_tokenize(row['Summarized_content']) \n",
    "        for sentence in sent_text:\n",
    "            clean_sentence = get_only_chars(sentence)\n",
    "            tokenized_text.append(nltk.word_tokenize(clean_sentence))\n",
    "        print(num_augmented_set)\n",
    "        for i in range(num_augmented_set):\n",
    "            text_augmentation_pipeline.Augmented_sentences = []\n",
    "            options = [1, 2, 3, 4, 5]\n",
    "            for sent in range(len(tokenized_text)):\n",
    "                rand_choice = random.choice(options)\n",
    "            \n",
    "                if rand_choice==1:                \n",
    "                    synonym_replacement(tokenized_text[sent])\n",
    "\n",
    "                elif rand_choice==2:                    \n",
    "                    random_swap(tokenized_text[sent])\n",
    "\n",
    "                elif rand_choice==3:                    \n",
    "                    random_insertion(tokenized_text[sent])\n",
    "\n",
    "                elif rand_choice==4:                    \n",
    "                    random_deletion(tokenized_text[sent])\n",
    "\n",
    "                elif rand_choice==5:                    \n",
    "                    similar_word_replacement(tokenized_text[sent])\n",
    "    \n",
    "            Augmented_data_with_LegalDetails = [{'Summarized_content':' '.join(text_augmentation_pipeline.Augmented_sentences), \n",
    "                                                 'Labels':np.int64(row['Labels']), 'DocID':row['DocID'],\n",
    "                                                 'Legal_Details':row['Legal_Details'], 'Filename':row['Filename']}]\n",
    "            Df_Augmented_files_dict = pd.DataFrame.from_dict(Augmented_data_with_LegalDetails)\n",
    "            Df_Augmented_files=Df_Augmented_files.append(Df_Augmented_files_dict, ignore_index=True, sort=False)\n",
    "\n",
    "    return Df_Augmented_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of equation - 4.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_augmentation(Sumy_csv, Sumy_Label_df):\n",
    "    augmented_files_II = pd.DataFrame()\n",
    "    \n",
    "    for _label in range(1,16):\n",
    "        Truncated_data = Sumy_csv[Sumy_csv['Labels'] == _label]\n",
    "        Ni_max = Sumy_Label_df.Records.max()\n",
    "        fraction = (Ni_max/len(Truncated_data))-1\n",
    "        Part_truncated_data = Truncated_data.sample(frac=fraction, random_state=rng)\n",
    "        Df_Augmented_files_II = text_augmentation_pipeline(Part_truncated_data, 'WoAS')\n",
    "        augmented_files_II=augmented_files_II.append(Df_Augmented_files_II, ignore_index=True, sort=False)\n",
    "        \n",
    "    return augmented_files_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_dist_plot(df):\n",
    "    data_dict = dict(df['Labels'].value_counts())\n",
    "    plt.rcParams[\"figure.figsize\"] = (25,10)\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "    bars = plt.bar(range(len(data_dict)), list(data_dict.values()), align='center', color = 'b', width=0.7) #list('rgbkymc')\n",
    "    plt.xticks(range(len(data_dict)), list(data_dict.keys()))\n",
    "    plt.xlabel(\"Document Classes\")\n",
    "    plt.ylabel(\"No. of samples\")\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x(), yval, yval)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    Word2Vec_model = Word2Vec.load('Thesis - Dataset and Transformations/word2vec/Word2Vec_100d.model')\n",
    "    # For tfidf file: Thesis - Dataset and Transformations/transform - post summarization/LSA_tfidf.csv\n",
    "    # For term freq: Thesis - Dataset and Transformations/transform - post summarization/LSA_frequency.csv\n",
    "    Sumy_csv = pd.read_csv('Thesis - Dataset and Transformations/transform - post summarization/LSA_binary.csv')\n",
    "    \n",
    "    Sumy_Label_df = aug_set_calculator(Sumy_csv)\n",
    "    augmented_files = text_augmentation_pipeline(Sumy_csv, 'WAS')\n",
    "    augmented_files_II = partial_augmentation(Sumy_csv, Sumy_Label_df)\n",
    "    \n",
    "    total_training_files = pd.concat([augmented_files, augmented_files_II, Sumy_csv], ignore_index=True)\n",
    "    # Change the saving path for each form of summarized document input.\n",
    "    # For tfidf: Thesis - Dataset and Transformations/transform - post text augmentation/lsa_tfidf_augmentation.csv\n",
    "    # For term freq: Thesis - Dataset and Transformations/transform - post text augmentation/lsa_tf_augmentation.csv\n",
    "    total_training_files.to_csv('Thesis - Dataset and Transformations/transform - post text augmentation/lsa_binary_augmentation.csv')\n",
    "    dataset_dist_plot(total_training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
