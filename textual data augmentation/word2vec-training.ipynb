{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from timeit import default_timer as timer\n",
    "nlp = spacy.load('en', disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(doc):\n",
    "    doc = nlp(doc)\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(txt)\n",
    "    \n",
    "    \n",
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    '''\n",
    "    Figure 4.6: TNSE plot for similar words\n",
    "    Function: Generates a t-Distributed Stochastic Neighbor Embedding graph to view the goodness of the model by analysing the\n",
    "    distribution of the high-dimensional data by assigning each data point to a location on a two dimensional plane.\n",
    "    '''\n",
    "    \n",
    "    arr = np.empty((0,100), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def word2vec_training(tokens):\n",
    "    '''\n",
    "    SECTION - 4.4.3 - Similar Word Replacement\n",
    "    Function: Defines the word2vec model with its hyperparameters, builds the vocabulary from the dataset and finally saves the \n",
    "    model at a desired location.\n",
    "    '''\n",
    "    start = timer()\n",
    "    print('Generating word embeddings...')\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    print('- using {} out of {} CPU cores'.format(cores - 1, cores))\n",
    "    \n",
    "    # Defining the hyperparameters for the word2vec model. \n",
    "    model = Word2Vec(min_count=10,  \n",
    "                    window=5,\n",
    "                    size=100,  \n",
    "                    sample=6e-5,  \n",
    "                    alpha=0.03,\n",
    "                    min_alpha=0.0007,\n",
    "                    workers=cores - 1)\n",
    "\n",
    "    # build the vocabulary table\n",
    "    print('- building the vocabulary table')\n",
    "    model.build_vocab(tokens, progress_per=10000)\n",
    "\n",
    "    # model training\n",
    "    print('- training the word2vec model')\n",
    "    model.train(tokens, total_examples=model.corpus_count, epochs=20, report_delay=1)\n",
    "\n",
    "    model.save('Thesis - Dataset and Transformations/word2vec/Word2Vec_100d.model')\n",
    "    #print('- word2vec model saved at {}'.format(path))\n",
    "\n",
    "    end = timer()\n",
    "    print('- took {:.2f} minutes'.format((end - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    tokenized_dataset = list()\n",
    "    #Iterate over each file in each folder in order.\n",
    "    for file in glob.glob(r\"Thesis - Dataset and Transformations/doc2vec/Train Docs.zip/*\"):\n",
    "        with open(file, 'r', encoding='utf8', errors= 'ignore') as infile:\n",
    "            doc_content = lemmatization(infile.read())\n",
    "        tokenized_dataset.append(doc_content.split())\n",
    "    \n",
    "    word2vec_training(tokenized_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
